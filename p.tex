\documentclass[a4paper,10pt,conference,compsocconf,final]{IEEEtran}

\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[stretch=10]{microtype} 
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage[mathscr]{eucal}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{xspace}
\usepackage{shortvrb}
\usepackage{mdwmath} 
\usepackage{verbatim}
\usepackage{multirow}
\usepackage[square,comma,numbers,sort]{natbib}
\usepackage{times}
\renewcommand{\ttdefault}{cmtt}

\usepackage[tight,footnotesize]{subfigure}
\newcommand\method[1]{{\sf\small{#1}}}
\newcommand{\opstyle}[1]{\mbox{\textsc{#1}}}
\newcommand{\var}[1]{\mbox{\emph{#1}}}
\def\D{\hphantom{1}}
\def\C{\hphantom{1,}}
%-- Sizes 
\newcommand\kb[1]{$#1$\,kB}
\newcommand\mb[1]{$#1$\,MB} 
\newcommand\gb[1]{$#1$\,GB}
\newcommand\tb[1]{$#1$\,TB} 
%--- Latex hyphenation sucks, make it go away.
\hyphenpenalty=5000
\tolerance=1000

\newcommand{\myparagraph}[1]{\vspace*{1ex}\noindent{\textbf{#1.}}~}
\newcommand{\shane}[1]{\textrm{\textcolor{orange}{Shane says: #1}}}
\newcommand{\ko}[1]{\textrm{\textcolor{red}{Kevin says: #1\\}}}

\usepackage{todonotes}
\newcommand{\am}{\todo[author=AM, color=red!20!white,inline]}

\begin{document} % 

% paper title % can use linebreaks \\ within to get better formatting as desired 
\title{RMIT at the TREC 2015 LiveQA Track}
\author{\IEEEauthorblockN{%
(Authors listed in lexicographical order of the surnames) \\
Ruey-Cheng Chen,
J. Shane Culpepper,
Tadele Tadela Damessie,
Timothy Jones,\\
Ahmed Mourad,
Kevin Ong,
Falk Scholer,
Evi Yulanti}
\IEEEauthorblockA{RMIT University\\
School of CS\&IT\\ 
{\{1, 2, 3, 4, 5, 6, 7, 8\}@rmit.edu.au}}
}

\maketitle

\begin{abstract} 
This paper describes the four systems RMIT fielded fo 
the \opstyle{TREC} 2015 LiveQA task.
\end{abstract}

\begin{IEEEkeywords} 
XXX; YYY
\end{IEEEkeywords}

\section{Overview}
\label{overview}
In the TREC LiveQA 2015 challenge, we experimented with four
different retrieval-based answer-finding strategies.
Instead of pursuing a traditional question-answering approach that
seeks deeper understanding to the questions, we focused on simple
enhancements, such as summarization, query trimming, and headword
expansion.
These are common techniques used in IR, and can relatively easily be
integrated into a research-purpose retrieval engines or pipelines of
a similar scale.
In our experiment, we considered the following research
questions:

\myparagraph{RQ 1:}
{\emph{
Will shorter or longer summaries result in better answers?
}}

\myparagraph{RQ 2:}
{\emph{
Should all of the terms in a question be used, or should only a 
subset of ``important'' terms be used?
}}

\myparagraph{RQ 3:}
{\emph{
Can headword expansion using external resources improve the
quality of answers?
}}

\bigskip

To answer these questions, we configured four different systems 
in the 2015 challenge.
Surprisingly, we found that passage retrieval using the full query
plus minimal summarization without query reduction or expansion
produced the best result in general.

\section{Data and Retrieval Settings}
We now describe the collection and retrieval setting used in our
system. 

\begin{figure*}
  \centering
  \includegraphics[width=0.90\textwidth]{TREC-live-QA.png}
  \label{fig:arch}
  \caption{System architechture for each RMIT system. Green shading indicates components that are different when compared to RMIT-0.}
\end{figure*}

\subsection{Server architechture}
\ko{Just remove this, if pointless.}
We allocated the computing resources from NecTAR\footnote{\url{https://www.nectar.org.au}} - an Australian National Research cloud computing network. Administrative access to the server are granted based on Public-Key Cryptography Standards (RFC3447\footnote{https://tools.ietf.org/html/rfc3447}). 

Question responses were dependent on the server ID when the questions were served (see Figure~\ref{fig:arch}). In the basic form, the server convert the questions into a query form which is then matched against the in-memory indices. 
\ko{Did we retrieve top-1 or top-k?}
The top-k relevant document retrieved are then submitted to the summariser component which outputs a number of sentences ranked by relevance according to the summarizer (see Section \ref{sec:sum}). 

In our final iteration, we wanted to see if subset of "important" terms and providing headword expansion will improve the performance. In this iteration, we extract key terms from the original question, which is then trimmed and performed a query expansion using word2vec. The trimmed form with word2vec expansion is passed to the query processor. The retrieved documents were then summarised by the summarizer component in which the first sentence is then set as response.

The various services were connected together using Go Programming Language. It included graceful handling of timeouts, allowing us to ensure
a response within the 60 second window. 

For the curious reader, our code is
available under a BSD license\footnote{\url{https://github.com/TimothyJones/trec-liveqa-server}}. Please cite this paper if you use the code for anything.

\subsection{Run descriptions}
\label{sec:runs}

\noindent\textbf{RMIT0\footnote{Originally referred to as Monash-System2 in the LiveQA challenge.} (automatic): }
Indri bag-of-words query for all terms in the query, and the default
document summarizer as described in Section~{\ref{sec:sum}.

\medskip

\noindent\textbf{RMIT1 (automatic): }
Indri bag-of-words query for all terms in the query, and the default
document summarizer as described in Section~{\ref{sec:sum}.
However, only the first sentence generated by the summary process
was returned.

\medskip

\noindent\textbf{RMIT2 (automatic): }
Indri bag-of-words query for all terms in the query, and the default
document summarizer as described in Section~{\ref{sec:sum}.
Query expansion was done using \method{word2vec}, and the original query terms were trimmed.

\medskip

\noindent\textbf{RMIT3 (automatic): }
Indri bag-of-words query for all terms in the query, and the default
document summarizer as described in Section~{\ref{sec:sum}.
Query expansion was done using \method{wordnet}, and the original query words were trimmed.

\medskip


\subsection{Collection}

\begin{table*}[!t]
\centering
\caption{Summary of collections indexed to answer questions.\label{tbl:col}}
\begin{tabular}{p{35mm}ccp{50mm}}
\toprule
{\bf Collection} & {\bf Number of Documents} & {\bf Number of Words} & {\bf Description} \\
\midrule
AQUAINT & $\D1{,}034$K & $\C506$M & Newswire, 1999 - 2000 \\
AQUAINT2 & $\D\C907$K & $\C410$M & Newswire, Oct 2004 - Mar 2006 \\
Wikipedia-EN & $\D4{,}847$K & $1{,}775$M & Online Knowledge Base\footnote{We used an enwiki dump produced on May 15, 2015 to create the document collection.} \\
Yahoo! Answers CQA v1.0 & $31{,}972$K & $1{,}462$M & Question answers converted to documents from the Yahoo! Answers website.
\\
\bottomrule
\end{tabular}
\end{table*}

Table~{\ref{tbl:col}} summarizes the collections we used in the task.  We
indexed AQUAINT and AQUAINT-2 as they were.  To prepare the data set for
English Wikipedia, we used the open-source tool {\method{wp-download}}
\footnote{\url{https://github.com/babilen/wp-download}}, to fetch a dump, extracted
all of the XML content using
{\method{wikiextractor}}\footnote{\url{https://github.com/attardi/wikiextractor}},
and indexed every wikipedia page as a document.  To index the Yahoo!  Answers
CQA data, we processed the collection as follows: rather than just indexing the
best answers, we extracted and indexed only the answers to previous questions
and stored them as documents.  We did not make use of the subject and content
tags (i.e., question title and description) in the data.  

\subsection{Indexing}
We used Indri 5.9 as
our retrieval engine with Krovetz stemming and the default InQuery
stoplist.\footnote{\url{http://www.lemurproject.org/indri.php}} Indexing the
collection described in the previous section, we ended up with a
single index of \gb{39} in file size that contained $38.7$ million documents
and $12.2$ million unique terms.

\subsection{Passage Retrieval}
\label{sec:passage}

According to our prior tests over the live stream, the question title is on
average sized 10 terms and the body roughly 30 terms.  As some of the question
body can be quite long, we decided to form the initial query intent using
the title only.  In two of our submitted runs, we tried different ways of
expressing the same query intent by adding/removing terms.  The details are
given in Section~\ref{sec:runs}.

We used the fixed-sized passage operator \texttt{\#combine[passage100:50](\ldots)} 
provided by Indri to retrieve and parse the top 3 passages from
document texts.  The result then got sent to the summarizer in the next stage.
For performance reasons, and the length of some of the queries, we ran a
bag-of-words query, and BM25 ranking.  For BM25, our parameter configuration
was $k_1=0.9$ and $b=0.4$.  \footnote{The values for $b$ and $k_1$ are
different than the defaults reported by {\citet{rwj+94-trec}}.  These parameter
choices were reported for Atire and Lucene in the 2015 IR-Reproducibility
Challenge, see {\url{github.com/lintool/IR-Reproducibility}} for further
details.}  

\subsection{Summarization}
\label{sec:sum}

We used the model proposed by Takamura and Okumura \cite{takamura2009text} to
generate extractive summaries from the top-ranked passages.  In this model,
summarization is characterized as a two-way optimization problem, in which one
seeks to maximize coverage over important words and minimize redundancies
simultaneously.  The mathematical formulation is given as follows:
\begin{equation}
\begin{split}
  \textrm{maximize} \qquad & (1-\lambda) \sum_{j} w_j z_j + \lambda \sum_{i}\sum_{j} x_i w_j a_{ij} \\
  \textrm{subject to} \qquad 
  & x_i \in \{0,1\}, z_j \in \{0,1\} \textrm{for all $i, j$}; \\
       & \sum_{i} c_ix_i \le K; \\
       & \sum_{i}^{} a_{ij}x_i \ge z_j \textrm{for all $j$} 
\end{split}
\end{equation}

To produce an extractive summary, one basically makes a choice over the set of
sentences and decides what to include.  By doing so, one also makes an implicit
choice over words.  This choice is modeled in the optimization problemd as two
sets of variables $x_i$ and $z_j$, the former indicating the binary decision on
keeping sentence $i$ and the latter that on keeping word $j$ in the summary.
In other words, for each sentence $i$, $x_i$ is set to 1 if sentence $i$ is to
be included in the summary or 0 otherwise.  Analogously for each term $j$,
$z_j$ is set to 1 if term $j$ is to be included.

In this problem, $c_i$ denotes cost of selecting sentence $s_i$ (i.e. number of
words in $s_i$), and $w_j$ denotes the weight of word $j$.  We used a tf-idf
weighting scheme in which term frequecy ($tf$) is pulled from question title
and body, and inverse document-freqency ($idf$) learned from a background
corpus.  Term frequency collected from question body is further penalized with
a factor $\alpha < 1$ as the information given in question body can be less
precise than in title.
\begin{equation}
  w_j = \left[ tf_{title}(j) + \alpha ~ tf_{body}(j) \right] * idf(j)
\end{equation}

The correspondence between sentence $i$ and word $j$ is coded in the indicator
variable $a_{ij}$, whose value is set to 1 if word $j$ appears in sentence $i$
and 0 otherwise.  With the first constraint, we limit the size of summary to
$K$ words at most.  With the second constraint, the word coverage is related to
the sentence coverage, thus completing the formulation.

Empirically, we fine-tune parameters $\lambda$ and $\alpha$ based on the prior
runs.  In the challenge, we set $\lambda = 0.1$ and $\alpha = 0.43$.  We use
IBM CPLEX solver to compute the optimal alloation.
	
\subsection{Headword Detection}
\label{sec:head}

TBA

\vskip7em

\subsection{Query Trimming and Headword Expansion}
\label{sec:redexp}

We experimented with a feature called query trimming, which is to use only the
``important'' terms in the question title to retrieve answers.  As the
questions from Yahoo! Answers are quite verbose, intended to be more
comprehensible than effective in retrieval, it makes sense to check if using
only some part of the query, especially that contributes the most to ranking,
would result in some improvements in effectiveness.

We used the algorithms in Petri et al.~\cite{petri2013exploring,petri2014score}
to extract the maxscore $U_b$ for each term.\footnote{The code is available at
\url{https://www.github.com/jsc/WANDbl}.}  The maxscore is loaded into memory
whenever a server starts, and in run-time we used it to trim the initial query
down to some predefined size.  The size is set to 5 terms throughout the
experiments where trimming is applied.

We also experimented with two ways of expanding the headword externally, by
drawning information from resources such as \emph{word2vec} and \emph{wordnet}.  

In the former method, we took a pre-trained word embedding model distributed
with \method{word2vec}\footnote{\url{https://code.google.com/p/word2vec}} and
used \method{gensim}\footnote{\url{https://radimrehurek.com/gensim}} to
populate a list of query terms that are most similar to the given input.  

In the latter method, we implemented the models proposed by \cite{huang2008question, silva2011symbolic}. First we extract the head word which is supposed to be the most important term in the question in terms of helping to retrieve only relevant documents. Second, we extract its hypernyms using WordNet. Finally, we use the context of the question, excluding the head word, to resolve the semantic ambiguity of different senses and populate a list of synonyms. For example, considering the question \textit{What are the sales goals daily and monthly at MAC Cosmetics?}, the head word is `sales' and its hypernyms given the context are `gross sales', `income', `financial gain', and `gain sum'.

\vskip7em

\subsection{Results}

\begin{table*}[t]
\centering
\caption{
Effectiveness summary for all four RMIT systems when compared to the
average across all systems participating in the 2015 LiveQA track.
\label{tab:runs}}
\begin{tabular}{lcccccccccc}
\toprule
\multirow{2}{*}{\bf Run ID} & {\bf Avg. Score} && \multicolumn{4}{c}{\bf Success} && \multicolumn{3}{c}{\bf Precision} \\
& {\bf (0-3)} && {\bf @1+} & {\bf @2+} & {\bf @3+} & {\bf @4+} && {\bf @2+} & {\bf @3+} & {\bf @4+} \\
\midrule
RMIT0 &{\bf 0.663}&&$0.987$&{\bf 0.364}&{\bf 0.220}&{\bf 0.082}&&{\bf 0.369}&{\bf 0.223}&{\bf 0.083}\\
RMIT1 &$0.435$&&$0.992$&$0.267$&$0.130$&$0.039$&&$0.269$&$0.131$&$0.039$\\
RMIT2 &$0.378$&&{\bf 0.998}&$0.232$&$0.115$&$0.034$&&$0.232$&$0.115$&$0.034$\\
RMIT3 &$0.412$&&$0.994$&$0.251$&$0.126$&$0.038$&&$0.252$&$0.127$&$0.038$\\
&&&&&&&&&\\
All Runs  &$0.465$&&$0.925$&$0.262$&$0.146$&$0.060$&&$0.284$&$0.159$&$0.065$\\
\bottomrule
\end{tabular}
\end{table*}

Table~\ref{tab:runs} shows foo.

\section{Error Analysis}

\section{Conclusion}

We show foo, bar, and bam.

%\bibliographystyle{IEEEtran} 
\bibliographystyle{plainnat} 
\bibliography{p}

\end{document}
