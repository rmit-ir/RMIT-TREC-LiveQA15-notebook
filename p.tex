\documentclass[a4paper,10pt,conference,compsocconf,final]{IEEEtran}

\usepackage[hyphens]{url}
\usepackage{hyperref}
\usepackage{color}
\usepackage[utf8]{inputenc}
\usepackage[stretch=10]{microtype} 
\usepackage{graphicx}
\usepackage{booktabs} 
\usepackage[mathscr]{eucal}
\usepackage{mathtools}
\usepackage{enumerate}
\usepackage{xspace}
\usepackage{shortvrb}
\usepackage{mdwmath} 
\usepackage{verbatim}
\usepackage{multirow}
\usepackage[square,comma,numbers,sort]{natbib}
\usepackage{balance}
\usepackage{times}
\renewcommand{\ttdefault}{cmtt}

\usepackage[tight,footnotesize]{subfigure}
\newcommand\method[1]{{\sf\small{#1}}}
\newcommand{\opstyle}[1]{\mbox{\textsc{#1}}}
\newcommand{\var}[1]{\mbox{\emph{#1}}}
\def\D{\hphantom{1}}
\def\C{\hphantom{1,}}
%-- Sizes 
\newcommand\kb[1]{$#1$\,kB}
\newcommand\mb[1]{$#1$\,MB} 
\newcommand\gb[1]{$#1$\,GB}
\newcommand\tb[1]{$#1$\,TB} 
%--- Latex hyphenation sucks, make it go away.
\hyphenpenalty=5000
\tolerance=1000

%

\newcommand{\myparagraph}[1]{\vspace*{1ex}\noindent{\textbf{#1.}}~}
\newcommand{\shane}[1]{\textrm{\textcolor{orange}{Shane says: #1}}}
\newcommand{\ko}[1]{\textrm{\textcolor{red}{Kevin says: #1\\}}}

\usepackage{todonotes}
\newcommand{\am}{\todo[author=AM, color=red!20!white,inline]}

\begin{document} % 

% paper title % can use linebreaks \\ within to get better formatting as desired 
\title{RMIT at the TREC 2015 LiveQA Track}
\author{\IEEEauthorblockN{%
(Authors listed in lexicographical order of the surnames) \\
Ruey-Cheng Chen,
J. Shane Culpepper,
Tadele Tadela Damessie,
Timothy Jones,\\
Ahmed Mourad,
Kevin Ong,
Falk Scholer,
Evi Yulanti}
\IEEEauthorblockA{RMIT University\\
School of CS\&IT\\ 
{\{ruey-cheng.chen, shane.culpepper\}@rmit.edu.au, s3497203@student.rmit.edu.au,} \\
{\{timothy.jones, ahmed.mourad, kevin.ong, falk.scholer, evi.yulanti\}@rmit.edu.au}
}}

\maketitle

\begin{abstract} This paper describes the four systems RMIT fielded
for the {\opstyle{TREC}} 2015 LiveQA task and the associated
experiments.
The challenge results show that the base run RMIT-0 has achieved an
above-average performance, but other attempted improvements have all
resulted in decreased retrieval effectiveness.
\end{abstract}

\begin{IEEEkeywords} 
  \opstyle{TREC} LiveQA 2015; RMIT; passage retrieval; summarization; query trimming; headword expansion
\end{IEEEkeywords}

\section{Overview}
\label{overview}
In the TREC LiveQA 2015 challenge, we experimented with four
different retrieval-based answer-finding strategies.
Instead of pursuing a traditional question-answering approach that
seeks deeper understanding of the questions, we focused on simple
enhancements, such as summarization, query trimming, and headword
expansion.
These are common techniques used in IR, and can easily be
integrated into research-purpose retrieval engines or pipelines of
a similar scale.
In our experiments, we considered the following research questions: 

\myparagraph{RQ 1:}
{\emph{
Will shorter or longer summaries result in better answers?
}}

\myparagraph{RQ 2:}
{\emph{
Should all of the terms in a question be used, or should only a 
subset of ``important'' terms be used?
}}

\myparagraph{RQ 3:}
{\emph{
Can headword expansion using external resources improve the
quality of answers?
}}

\bigskip

To answer these questions, we configured four different systems 
in the 2015 challenge.
Surprisingly, we found that passage retrieval using the full query
with minimal summarization and no query reduction or expansion
produced the best results.

\section{Data and Retrieval Settings}
We now describe the collection and retrieval setting used in our
system. 

\begin{figure*}
  \centering
  \includegraphics[width=0.90\textwidth]{TREC-live-QA.png}
  \label{fig:arch}
  \caption{System architecture for each RMIT system.
  Green shading indicates components that are different when compared
  to RMIT-0.}
  \end{figure*}

\subsection{Server architecture}
% \ko{Just remove this, if pointless.}
% We allocated the computing resources from NecTAR\footnote{\url{https://www.nectar.org.au}} - an Australian National Research cloud computing network. Administrative access to the server are granted based on Public-Key Cryptography Standards (RFC3447\footnote{https://tools.ietf.org/html/rfc3447}). 

The servers are built on top of the computing resources we allocated
from NecTAR,\footnote{\url{https://www.nectar.org.au}} the Australian
National Research cloud computing network.
Throughout the challenge, we use only one instance to host all of the
services.

Question responses were dependent on the server ID when the questions
were served (see Figure~\ref{fig:arch}).
In the most basic form, the server converted the questions into a bag-of-words
query, and ran these against the indexes.
The three most relevant passages retrieved are then submitted to our 
summarizer component, which outputs a predefined number of sentences ranked by
relevance within the summarizer (see Section \ref{sec:sum}).

In our final iteration, we wanted to see if a subset of ``important''
terms derived from a headword expansion would improve performance.
In this iteration, we extracted key terms from the original question,
which were then trimmed.
Query expansion of the headwords was carried out using {\method{word2vec}}.
The reduced query with {\method{word2vec}} headword expansion is
passed to the query processor.
The retrieved documents were then summarized by the summarizer, and
the first sentence is then returned as the response.

The various services were connected using a resource allocator
written in the Go Programming Language.
It included graceful handling of timeouts, and guaranteed responses
within the 60 second window.
For the curious reader, our code is available under a BSD
license.\footnote{\url{https://github.com/TimothyJones/trec-liveqa-server}}
Please cite this paper if you use the code for anything.

\subsection{Run descriptions}
\label{sec:runs}

\noindent\textbf{RMIT-0\footnote{Originally referred to as
Monash-System2 in the LiveQA challenge.}
(automatic): } Indri bag-of-words passage retrieval using all of the
terms in the question title, and top three passages summarized by the method
described in Section~{\ref{sec:sum}.

\medskip

\noindent\textbf{RMIT-1 (automatic): } Indri bag-of-words passage
retrieval using all of the terms in the question title, and the
top three passages summarized by the method described in
Section~{\ref{sec:sum}.
However, only the first sentence generated by the summary process was
returned.

\medskip

\noindent\textbf{RMIT-2 (automatic): } Indri bag-of-words passage
retrieval using terms derived from the question title, term trimming (down to 5 terms),
headword expansion (adding up to 5 terms) using {\method{word2vec}} as described in
Section~{\ref{sec:redexp}}, and the top three passages summarized by
the method described in Section~{\ref{sec:sum}.
Only the first sentence generated by the summary process was
returned.

\medskip

\noindent\textbf{RMIT-3 (automatic): } Indri bag-of-words passage
retrieval using terms derived from the question title, term trimming (down to 5 terms),
headword expansion (adding up to 5 terms) using \method{wordnet} as described in 
Section~{\ref{sec:redexp}}, and top three passages summarized by the
method described in Section~{\ref{sec:sum}.  Only the first
sentence generated by the summary process was returned.

\medskip

\begin{table*}[!t]
\centering
\caption{Summary of collections indexed to answer questions.\label{tbl:col}}
\begin{tabular}{p{35mm}ccp{50mm}}
\toprule
{\bf Collection} & {\bf Number of Documents} & {\bf Number of Words} & {\bf Description} \\
\midrule
AQUAINT & $\D1{,}034$K & $\C506$M & Newswire, 1999 - 2000 \\
AQUAINT2 & $\D\C907$K & $\C410$M & Newswire, Oct 2004 - Mar 2006 \\
Wikipedia-EN & $\D4{,}847$K & $1{,}775$M & Online knowledge base \\
Yahoo! Answers CQA v1.0 & $31{,}972$K & $1{,}462$M & Question answers converted to documents from the Yahoo! Answers website.
\\
\bottomrule
\end{tabular}
\end{table*}

\subsection{Collection}

Table~{\ref{tbl:col}} summarizes the collections we used in the task.  We
indexed AQUAINT and AQUAINT-2 as TREC Text documents.
To prepare the data set for English Wikipedia, we used the
open-source tool {\method{wp-download}},
\footnote{\url{https://github.com/babilen/wp-download}} to fetch a
dump,\footnote{We used an enwiki dump produced on May 15, 2015.} extracted all of the XML content using
{\method{wikiextractor}},\footnote{\url{https://github.com/attardi/wikiextractor}}
and indexed every Wikipedia page as a document.
To index the Yahoo! Answers CQA data, we processed the collection as 
follows: rather than just indexing the best answers, we extracted and
indexed only the answers to previous questions and stored them as
documents.
We did not make use of the subject and content tags (i.e., question
title and description) in the data.

\subsection{Indexing}
We used Indri 5.9 as our retrieval engine with Krovetz stemming and
the default InQuery
stoplist.\footnote{\url{http://www.lemurproject.org/indri.php}}
Once the collection described in the previous section was indexed, we 
ended up with a single \gb{39} index that contained $38.7$
million documents and $12.2$ million unique terms.

\subsection{Passage Retrieval}
\label{sec:passage}

According to our prior tests over the live stream, the question title
contains 10 terms on average, and the body size is around 30 terms.
However, as some of the question bodies can be quite long, we decided to 
construct queries using only the title.
In two of our submitted runs, we tried different ways of expressing
the same query intent by doing query reduction and expansion.
The details of this approach are described in Section~\ref{sec:runs}.

We used the fixed-sized passage operator
\texttt{\#combine[passage100:50](\ldots)} provided by Indri to
retrieve and parse the top three passages from document texts.
The result was then sent to the summarizer.
For performance reasons, and the length of some of the queries, we
used a bag-of-words query, and BM25 ranking.
For BM25, our parameter configuration was $k_1=0.9$ and $b=0.4$.\footnote{The values for $b$ and $k_1$ are different than the
defaults reported by {\citet{rwj+94-trec}}.
These parameter choices were reported for Atire and Lucene in the
2015 IR-Reproducibility Challenge, see
{\url{github.com/lintool/IR-Reproducibility}} for further details.}

\subsection{Summarization}
\label{sec:sum}

For summarization, we used the model proposed by Takamura and Okumura
\cite{takamura2009text} to generate extractive summaries from the
top-ranked passages.
In this model, summarization is characterized as a two-way
optimization problem, in which coverage over important words is
maximized, and redundancies are minimized simultaneously.
The mathematical formulation is given as follows:
\begin{equation}
\begin{split}
  \textrm{maximize} \qquad & (1-\lambda) \sum_{j} w_j z_j + \lambda \sum_{i}\sum_{j} x_i w_j a_{ij} \\
  \textrm{subject to} \qquad 
       & x_i \in \{0,1\} \textrm{for all $i$}; \\ 
       & z_j \in \{0,1\} \textrm{for all $j$}; \\
       & \sum_{i} c_ix_i \le K; \\
       & \sum_{i}^{} a_{ij}x_i \ge z_j \textrm{for all $j$} 
\end{split}
\end{equation}

To produce an extractive summary, one basically makes a choice over
the set of sentences and decides what to include.
By doing so, one also makes an implicit choice over words.
This choice is modeled in the optimization problem as two sets of
variables $x_i$ and $z_j$, the former indicating the binary decision
on keeping sentence $i$, and the latter on keeping word $j$ in
the summary.
In other words, for each sentence $i$, $x_i$ is set to $1$ if sentence
$i$ is to be included in the summary, or $0$ otherwise.
Analogously for each term $j$, $z_j$ is set to $1$ if term $j$ is
included.

In this problem, $c_i$ denotes the cost of selecting sentence $s_i$ (i.e.
number of characters in $s_i$), and $w_j$ denotes the weight of word $j$.
We used a TF-IDF weighting scheme in which the term frequency ($\var{tf}$) is
derived from the question title and body, and the inverse document-frequency
($\var{idf}$) is learned from a background corpus.
The term frequency collected from the question body is further penalized with
a factor $\alpha < 1$ as the information given in the question body can
be less precise than in the title.
\begin{equation}
  w_j = \left[ \var{tf}_{\var{title}}(j) + \alpha ~ \var{tf}_{\var{body}}(j) \right] * \var{idf}(j)
\end{equation}

The correspondence between the sentence $i$ and the word $j$ is coded in the
indicator variable $a_{ij}$, whose value is set to $1$ if the word $j$ appears
in sentence $i$, and $0$ otherwise.  With the first constraint, we limit the
size of the summary to $K$ characters at most ($K$ is set to 1,000 throughout).
With the second constraint, the word coverage is related to the sentence
coverage, thus completing the formulation.

Empirically, we fine-tuned the parameters $\lambda$ and $\alpha$ based on prior
test runs.  In the challenge, we set $\lambda = 0.1$ and $\alpha = 0.43$.  We
used the IBM CPLEX solver to compute the optimal allocation.

\input{tbl-results.tex}


\subsection{Headword Detection}
\label{sec:head}

A {\em headword} is the key term in the question that helps
to retrieve the most relevant documents.
For example, consider the question: {\tt What are the sales
goals daily and monthly at MAC Cosmetics?}.
Here, the headword is {\tt sales}.
The process of generating the hypernyms is divided into two stages.
To extract the headword, we generate the syntactic parse tree of the
question using the Stanford parser.
Then, we apply the rule-based model initially defined by Collins
\cite{collins2003head} and later refined by Huang et al.
\cite{huang2008question} and Silva et al. \cite{silva2011symbolic}.



\subsection{Query Trimming and Headword Expansion}
\label{sec:redexp}

We also experimented with a feature called query trimming, which is to use
only the ``important'' terms in the question title to retrieve
answers.
The questions from Yahoo! Answers are quite verbose, and intended 
to be human readable.
However, verbose queries are known to perform poorly in many keyword-based
IR systems.
So, it seems sensible to use only a subset of terms from the question,
especially if the terms selected are likely to contribute the most in the
ranking function.

First, we used the WAND implementation from Petri et
al.~\cite{petri2013exploring,petri2014score} to extract the MaxScore
$U_b$ for each term.\footnote{The code is available at
\url{https://www.github.com/jsc/WANDbl}.}
The MaxScore list is then loaded into memory when the server starts.
At query time, we used the list to order terms by impact, and trim the
initial query down to a predefined size.
The size is set to five terms throughout the experiments where trimming
is applied.

We also experimented with two ways of expanding the headword term in
each query externally, by drawing information from resources such as
\method{word2vec} and \method{wordnet}:

\begin{itemize}
  \item \method{word2vec}:  
    In this method, we took a pre-trained word
    embedding model distributed with
    \method{word2vec}\footnote{\url{https://code.google.com/p/word2vec}} and
    used \method{gensim}\footnote{\url{https://radimrehurek.com/gensim}} to
    populate a list of query terms that are most similar to a given input.  

  \item \method{wordnet}: In this method, we implemented the models proposed by
    Huang et al. \cite{huang2008question} and Silva et al. \cite{silva2011symbolic}.  We extract the hypernyms of
    the head word using WordNet, and map the Penn Treebank POS Tags to WordNet
    tags to decide which part of speech senses should be considered.  Then,
    following the algorithm of Huang et al. \cite{huang2008question} for head word-sense disambiguation, we calculate terms overlap between the definition of
    each sense and the definition of context words (each word in the question
    excluding the head word) with maximum depth of six. The optimal sense (the one
    which results in the maximum number of common words) is chosen to populate the list
    of synonyms.  
\end{itemize}

Note that in the latter method, we use the context of the question, excluding
the headword, to resolve the semantic ambiguity of different senses and
populate a list of synonyms.  For example, consider again the question \textit{What
are the sales goals daily and monthly at MAC Cosmetics?}, the headword is
{\tt sales} and the hypernyms are {\tt gross sales}, {\tt income}, {\tt
financial gain}, and {\tt gain sum}.

For either approach, we added the top five generated expansion terms back into
the query without any modification.  Note that when query trimming is applied,
the query would first get trimmed down to 5 terms, and then expanded using the
headword to at most 10 terms totally.

\section{Results}

The LiveQA challenge results are given in Table~\ref{tab:runs}, where
our submitted runs and the average result across all runs are shown.
Our base run RMIT-0 delivered the best performance in our experiment,
achieving $0.663$ {in} Avg Score.
The base run outperforms the average across all runs submitted to the
challenge.
On the other hand, all refinements that we tested resulted in decreased
performance, below the average over all submitted runs.

Limiting the output to only the first sentence in the summary (RMIT-1)
appears to have a negative effect on precision at all relevance
level.
This is surprising, since adding more sentences increases the
likelihood that non-relevant information would appear in the summary.

Our result on query trimming and headword expansion also shows no improvement.
Therefore, this combined strategy is not effective when top-sentence precision
is of concern.  Among the two expansion methods, \method{word2vec} (RMIT-2)
appears to do more harm than \method{wordnet} (RMIT-3).  We speculated that
headword expansion using \method{word2vec} is not practically useful, as
\method{word2vec} can be too aggressive sometimes, generating terms that are not
synonyms to the headword and thus wildly biasing the original intent.

\section{Conclusion}

We have explored four different system configurations for the TREC
LIVEQA Track in 2015.
While we are pleasantly surprised with the performance of our
baseline system, we believe further improvements can still be realized
using query reduction and headword expansion.
We hope that further post-run analysis will provide insight into
why the approaches were not successful in our current system
configurations.

\myparagraph{Acknowledgment}
This work was supported by the Australian Research Council's
{\emph{Discovery Projects}} Scheme (DP140102655).
Shane Culpepper is the recipient of an Australian Research Council
DECRA Research Fellowship (DE140100275).


%\bibliographystyle{IEEEtran} 
\balance
\bibliographystyle{plainnat} 
\bibliography{p}

\end{document}
